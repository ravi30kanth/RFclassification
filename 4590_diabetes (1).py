# -*- coding: utf-8 -*-
"""4590 diabetes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ULKrNz8jP0KcUBDsqaHSrAVa04yxoH-0

# **What is Diabetes?**
Diabetes is a chronic disease that occurs when the pancreas is no longer able to make insulin, or when the body cannot make good use of the insulin it produces. Learning how to use Machine Learning can help us predict Diabetes. Letâ€™s get started!

# **About this project :-**
The objective of this project is to classify whether someone has diabetes or not.
Dataset consists of several Medical Variables(Independent) and one Outcome Variable(Dependent)
The independent variables in this data set are :-'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age'
The outcome variable value is either 1 or 0 indicating whether a person has diabetes(1) or not(0).

# **About the Dataset:**
Pregnancies :- Number of times a woman has been pregnant

Glucose :- Plasma Glucose concentration of 2 hours in an oral glucose tolerance test

BloodPressure :- Diastollic Blood Pressure (mm hg)

SkinThickness :- Triceps skin fold thickness(mm)

Insulin :- 2 hour serum insulin(mu U/ml)

BMI :- Body Mass Index ((weight in kg/height in m)^2)

Age :- Age(years)

DiabetesPedigreeFunction :-scores likelihood of diabetes based on family history)

Outcome :- 0(doesn't have diabetes) or 1 (has diabetes)

# **Importing** **Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

## reading the dataset
df=pd.read_csv('/content/diabetes (4).csv')

df.head()

df.tail()

df.shape

"""# **Exploratory Data Analysis (EDA)**"""

## checking the null values
df.isnull().sum()

df.info()

df.describe()

df.describe().T

"""# **Data Visualization**"""

##Plotting the data distribution plots before removing null values
p = df.hist(figsize = (20,20))

plt.figure(figsize=(20,15),facecolor='white')
plot_num=1


for column in df:
  if plot_num <= 9:
    ax=plt.subplot(3,3,plot_num)
    sns.distplot(df[column])
    plt.xlabel(column,fontsize=20)
  plot_num+=1
plt.show()

df.columns

# Check how many data point have 0 value
df[df[["Glucose","BloodPressure","SkinThickness","Insulin","BMI"]] <= 0].count()

df['Glucose'] = df['Glucose'].replace(0,df['Glucose'].mean())
df['BloodPressure'] = df['BloodPressure'].replace(0,df['BloodPressure'].mean())
df['SkinThickness'] = df['SkinThickness'].replace(0,df['SkinThickness'].median())
df['Insulin'] = df['Insulin'].replace(0,df['Insulin'].median())
df['BMI'] = df['BMI'].replace(0,df['BMI'].median())

df.describe()

df.describe().T

## Distribution of data after replacing the zero values with mean
p = df.hist(figsize = (20,20))

plt.figure(figsize=(20,15),facecolor='white')
plot_num=1


for column in df:
  if plot_num <= 9:
    ax=plt.subplot(3,3,plot_num)
    sns.distplot(df[column])
    plt.xlabel(column,fontsize=20)
  plot_num+=1
plt.show()

"""# **Handling Outliers**"""

## find the outlier in the dataset
fig, ax =plt.subplots(figsize=(15,10))
sns.boxplot(data=df, width=0.5)

sns.boxplot(df['Insulin'])

Q1 = df.Insulin.quantile(0.25)
Q3 = df.Insulin.quantile(0.75)
IQR = Q3-Q1
lower = Q1 - 1.5*IQR
upper = Q3 + 1.5*IQR
df.loc[df["Insulin"] > upper,"Insulin"] = upper

import seaborn as sns
sns.boxplot(x = df["Insulin"]);

sns.boxplot(df['BloodPressure'])

Q1 = df.BloodPressure.quantile(0.25)
Q3 = df.BloodPressure.quantile(0.75)
IQR = Q3-Q1
lower = Q1 - 1.5*IQR
upper = Q3 + 1.5*IQR
df.loc[df["BloodPressure"] > upper,"BloodPressure"] = upper
df.loc[df["BloodPressure"] < lower,"BloodPressure"] = lower

sns.boxplot(x = df["BloodPressure"]);

sns.boxplot(df['SkinThickness'])

Q1 = df.SkinThickness.quantile(0.25)
Q3 = df.SkinThickness.quantile(0.75)
IQR = Q3-Q1
lower = Q1 - 1.5*IQR
upper = Q3 + 1.5*IQR
df.loc[df["SkinThickness"] > upper,"SkinThickness"] = upper
df.loc[df["SkinThickness"] < lower,"SkinThickness"] = lower

sns.boxplot(df['SkinThickness'])

sns.boxplot(df['BMI'])

Q1 = df.BMI.quantile(0.25)
Q3 = df.BMI.quantile(0.75)
IQR = Q3-Q1
lower = Q1 - 1.5*IQR
upper = Q3 + 1.5*IQR
df.loc[df["BMI"] > upper,"BMI"] = upper
df.loc[df["BMI"] < lower,"BMI"] = lower

sns.boxplot(df['BMI'])

sns.boxplot(df['Age'])

Q1 = df.Age.quantile(0.25)
Q3 = df.Age.quantile(0.75)
IQR = Q3-Q1
lower = Q1 - 1.5*IQR
upper = Q3 + 1.5*IQR
df.loc[df["Age"] > upper,"Age"] = upper
df.loc[df["Age"] < lower,"Age"] = lower

sns.boxplot(df['Age'])

sns.boxplot(df['DiabetesPedigreeFunction'])

Q1 = df.DiabetesPedigreeFunction.quantile(0.25)
Q3 = df.DiabetesPedigreeFunction.quantile(0.75)
IQR = Q3-Q1
lower = Q1 - 1.5*IQR
upper = Q3 + 1.5*IQR
df.loc[df["DiabetesPedigreeFunction"] > upper,"DiabetesPedigreeFunction"] = upper
df.loc[df["DiabetesPedigreeFunction"] < lower,"DiabetesPedigreeFunction"] = lower

sns.boxplot(df['DiabetesPedigreeFunction'])

# The classes of the outcome variable were examined.
df['Outcome'].value_counts()

# Outcome countplot
sns.countplot(x = 'Outcome',data = df)

##Correlation between all the features
plt.figure(figsize=(12,10))
# showcase heatmap
p = sns.heatmap(df.corr(), annot=True,cmap ='RdYlGn')

"""# **TRAIN TEST SPLIT**"""

train = df.sample(frac=0.7, random_state=143)
test = df.drop(train.Outcome)

X_train = train.drop('Outcome', axis=1)
y_train = train['Outcome']
X_test = test.drop('Outcome', axis=1)
y_test = test['Outcome']

X_train.shape

X_test.shape

y_train.shape

y_test.shape

import numpy as np
from collections import Counter
import random

def gini_impurity(y):
    """Calculate the Gini impurity of a list of class labels."""
    counter = Counter(y)
    impurity = 1
    for label in counter:
        prob = counter[label] / len(y)
        impurity -= prob ** 2
    return impurity

def split_data(X, y, feature, threshold):
    """Split the data based on a feature and threshold."""
    left_X, left_y, right_X, right_y = [], [], [], []
    for i in range(len(X)):
        if X[i][feature] < threshold:
            left_X.append(X[i])
            left_y.append(y[i])
        else:
            right_X.append(X[i])
            right_y.append(y[i])
    return np.array(left_X), np.array(left_y), np.array(right_X), np.array(right_y)

def find_best_split(X, y):
    """Find the best feature and threshold to split the data."""
    best_feature, best_threshold, best_gini = None, None, 1
    for feature in range(X.shape[1]):
        thresholds = list(set(X[:, feature]))
        for threshold in thresholds:
            left_X, left_y, right_X, right_y = split_data(X, y, feature, threshold)
            if len(left_y) == 0 or len(right_y) == 0:
                continue
            gini = (len(left_y) / len(y)) * gini_impurity(left_y) + (len(right_y) / len(y)) * gini_impurity(right_y)
            if gini < best_gini:
                best_feature, best_threshold, best_gini = feature, threshold, gini
    return best_feature, best_threshold

class DecisionTree:
    """A decision tree for classification."""
    
    def __init__(self, max_depth=None):
        self.left = None
        self.right = None
        self.feature = None
        self.threshold = None
        self.label = None
        self.max_depth = max_depth
    
    def fit(self, X, y, depth=0):
        """Build the decision tree recursively."""
        if self.max_depth is not None and depth >= self.max_depth:
            self.label = Counter(y).most_common()[0][0]
            return
        if len(set(y)) == 1:
            self.label = y[0]
            return
        best_feature, best_threshold = find_best_split(X, y)
        self.feature = best_feature
        self.threshold = best_threshold
        left_X, left_y, right_X, right_y = split_data(X, y, best_feature, best_threshold)
        self.left = DecisionTree(max_depth=self.max_depth)
        self.right = DecisionTree(max_depth=self.max_depth)
        self.left.fit(left_X, left_y, depth+1)
        self.right.fit(right_X, right_y, depth+1)
    
    def predict(self, X):
        """Make predictions for a list of samples."""
        predictions = []
        for sample in X:
            node = self
            while node.label is None:
                if sample[node.feature] < node.threshold:
                    node = node.left
                else:
                    node = node.right
            predictions.append(node.label)
        return np.array(predictions)

class RandomForestClassifier:
    """A random forest classifier."""
    
    def __init__(self,n_trees=10, n_estimators=100, max_depth=None, max_features=None):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.max_features = max_features
    
    def fit(self, X, y):
        """Build the random forest classifier."""
        self.trees = []
        self.feature_indices = []
        n_features = X.shape[1]
        if self.max_features is None:
            self.max_features = int(np.sqrt(n_features))
        for i in range(self.n_trees):
            tree = DecisionTree(max_depth=self.max_depth)
            feature_indices = random.sample(range(n_features), self.max_features)
            self.feature_indices.append(feature_indices)
            tree_X = X[:, feature_indices]
            tree.fit(tree_X, y)
            self.trees.append(tree)
    
    def predict(self, X):
        """Make predictions for a list of samples."""
        predictions = []
        for i in range(self.n_trees):
            tree = self.trees[i]
            feature_indices = self.feature_indices[i]
            tree_X = X[:, feature_indices]
            tree_predictions = tree.predict(tree_X)
            predictions.append(tree_predictions)
        predictions = np.array(predictions)
        return np.apply_along_axis(lambda x: Counter(x).most_common()[0][0], axis=0, arr=predictions)

clf = RandomForestClassifier(n_estimators=100,max_depth=5,max_features=5)
clf.fit(X_train.values,y_train.values.ravel())

# Make predictions on the test data
y_pred = clf.predict(X_test.values)

def accuracy_score(y_true, y_pred):
    correct = 0
    for i in range(len(y_true)):
        if y_true[i] == y_pred[i]:
            correct += 1
    return correct / float(len(y_true))

accuracy = accuracy_score(y_test.values, y_pred)
print("Accuracy:", accuracy)

